{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:02<00:00, 160.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:02<00:00, 160.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.8999999985098839\n",
      "test error: 0.9020000025629997\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0], device='cuda:0') tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
    "\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def overlay_y_on_x(x, y):\n",
    "    \"\"\"\n",
    "    Replace the first 10 pixels of data [x] with one-hot-encoded label [y]\n",
    "    \"\"\"\n",
    "    x_ = x.clone()\n",
    "    x_[:, :10] *= 0.0\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
    "\n",
    "    def predict(self, x): \n",
    "        # 这个predict方法是理解ff方法的关键，它不是像普通的predict方法一样，输入一个样本，输出一个长度为num_cls的softmax预测向量\n",
    "        # 而是一个样本反复输入这个网络num_cls次，把每种带标签的可能都计算一个goodness，也就是这个数据是好数据的可能性，找出最高goodness的就是预测类别\n",
    "        goodness_per_label = []\n",
    "        for label in range(10): # 对每一个标签进行预测\n",
    "            h = overlay_y_on_x(x, label) # h是输入x和标签label的叠加\n",
    "            goodness = [] # goodness是一个列表，里面存放了每一层的结果向量的均方\n",
    "            for layer in self.layers: # 对每一层进行前传\n",
    "                h = layer(h) # h是每一层的输出\n",
    "                goodness += [h.pow(2).mean(1)] # goodness是每一层的结果向量的均方。h.pow(2)是h的每一个元素的平方，mean(1)是对每一行求均值\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)] # goodness_per_label是每一层的结果向量的均方的和\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1) # goodness_per_label是每一层的结果向量的均方的和的列表\n",
    "        return goodness_per_label.argmax(1) # 返回的是goodness_per_label中每一行最大值的索引，也就是说，返回的是每一行最大值的列索引\n",
    "\n",
    "    def train(self): #, x_pos, x_neg): # 这个train方法是对整个网络进行训练，训练的目标是让正样本的结果向量的均方上升，负样本的结果向量的均方下降\n",
    "        x, y = next(iter(train_loader))\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        x_pos = overlay_y_on_x(x, y)\n",
    "        # rnd = torch.randperm(x.size(0)) # 生成一个从0到n-1的随机整数序列。\n",
    "        # x_neg = overlay_y_on_x(x, y[rnd])\n",
    "        y_rnd = y.clone()\n",
    "        for i, y_i in enumerate(y):\n",
    "            li = list(range(10))\n",
    "            li.remove(y_i)\n",
    "            j = np.random.choice(li)\n",
    "            y_rnd[i] = j\n",
    "\n",
    "        x_neg = overlay_y_on_x(x, y_rnd)\n",
    "\n",
    "        h_pos, h_neg = x_pos, x_neg # h_pos和h_neg是正样本和负样本的输入\n",
    "        for i, layer in enumerate(self.layers): # 对每一层进行训练\n",
    "            print('training layer', i, '...') # 这里的i是层数\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg) # 对每一层进行训练，得到了正样本和负样本的结果向量，这个结果向量是该层的输出，也是下一层的输入\n",
    "            # 也就是说，这个训练的过程中，正样本在前传过程中得到的每一层输出都被认为是正的，负样本在前传过程中得到的每一层输出都被认为是负的，也就是说，出身决定一切\n",
    "\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=0.008)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 10000 # 训练的次数是1000次\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)  # 这个是对输入做了归一化，使得输入的模长为1，这在论文里有解释\n",
    "        x_direction = x / x.max()\n",
    "        x_direction =  self.relu(\n",
    "            torch.mm(x_direction, self.weight.T) +\n",
    "            self.bias.unsqueeze(0) # 这个是对输入做了最基本的前向传播，得到了结果向量\n",
    "            ) # 注意，在前传之后，随即使用了relu激活函数，这意味着每一层的所有激活值都是非负的\n",
    "        return x_direction\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        # 训练其实就是对每一层分别进行训练，训练的目标是让正样本的结果向量的均方上升，负样本的结果向量的均方下降\n",
    "        # 每一层的forward方法定义如上一个函数，这里的train方法定义了训练的过程\n",
    "        for i in tqdm(range(self.num_epochs)):\n",
    "            # minibatch\n",
    "            bz = 100\n",
    "            for j in range(0, x_pos.size(0), bz):\n",
    "                mask = torch.randperm(x_pos.size(0))[:bz]\n",
    "                h_pos = x_pos[mask] # 随机采样1000个正样本\n",
    "                h_neg = x_neg[mask] # 随机采样1000个负样本\n",
    "\n",
    "\n",
    "                # for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
    "                #     visualize_sample(data, name)\n",
    "                \n",
    "                # print(self.forward(x_pos).pow(2), self.forward(x_pos).pow(2).shape)\n",
    "                g_pos = self.forward(h_pos).pow(2).mean(1) # g_pos 是正样本x_pos在该层前向传播得到的结果向量的均方\n",
    "                g_neg = self.forward(h_neg).pow(2).mean(1) # g_neg 是负样本x_neg在该层前向传播得到的结果向量的均方\n",
    "                # 论文关于使用L2范数来度量的理由：\n",
    "                # There are two main reasons for using the squared length of the activity vector as the goodness function.\n",
    "                # First, it has very simple derivatives. Second, layer normalization removes all trace of the goodness.\n",
    "                \n",
    "                # The following loss pushes pos (neg) samples to\n",
    "                # values larger (smaller) than the self.threshold.\n",
    "                # 随着训练过程，loss下降，g_pos将上升，g_neg将下降\n",
    "                loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                    -g_pos + self.threshold,\n",
    "                    g_neg - self.threshold]))).mean() # loss = [log(1+exp(-(g_pos-threshold))) + log(1+exp(g_neg-threshold))] / 2\n",
    "                # print(loss)\n",
    "                self.opt.zero_grad()\n",
    "                # this backward just compute the derivative and hence\n",
    "                # is not considered backpropagation.\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                # 关于这里为什么能够work：\n",
    "                # 1. loss是权重的函数，loss的核心思想是让g_pos上升，g_neg下降\n",
    "                # 2. g_pos和g_neg是x_pos和x_neg的函数，x_pos和x_neg反映了客观世界，是这样要学习的对象。有了x_pos和x_neg，就能够计算出g_pos和g_neg，有了g_pos和g_neg，就能够计算出loss\n",
    "                # 3. 通过loss.backward()，计算loss对权重的梯度，使得loss下降，g_pos上升，g_neg下降\n",
    "                # 4. 通过self.opt.step()，更新了self.weight和self.bias\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
    "\n",
    "    \n",
    "# def visualize_sample(data, name='', idx=0):\n",
    "#     reshaped = data[idx].cpu().reshape(28, 28)\n",
    "#     plt.figure(figsize = (4, 4))\n",
    "#     plt.title(name)\n",
    "#     plt.imshow(reshaped, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(1234)\n",
    "    train_loader, test_loader = MNIST_loaders(train_batch_size=1000, test_batch_size=10000)\n",
    "\n",
    "    net = Net([784, 500, 500])\n",
    "    x, y = next(iter(train_loader))\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    # x_pos = overlay_y_on_x(x, y)\n",
    "    # # rnd = torch.randperm(x.size(0)) # 生成一个从0到n-1的随机整数序列。\n",
    "    # # x_neg = overlay_y_on_x(x, y[rnd])\n",
    "    # y_rnd = y.clone()\n",
    "    # for i, y_i in enumerate(y):\n",
    "    #     li = list(range(10))\n",
    "    #     li.remove(y_i)\n",
    "    #     j = np.random.choice(li)\n",
    "    #     y_rnd[i] = j\n",
    "\n",
    "    # x_neg = overlay_y_on_x(x, y_rnd)\n",
    "    # # for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
    "    # #     visualize_sample(data, name)\n",
    "    \n",
    "    net.train()#x_pos, x_neg)\n",
    "\n",
    "    print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "    x_te, y_te = next(iter(test_loader))\n",
    "    x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "\n",
    "    print('test error:', 1.0 - net.predict(x_te).eq(y_te).float().mean().item())\n",
    "\n",
    "    print(net.predict(x_te)[:30], y_te[:30])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
