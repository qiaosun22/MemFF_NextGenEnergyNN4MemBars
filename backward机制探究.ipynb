{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "np.random.choice(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch 动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先建立张量 `a` 、 `b` 和 `loss` 的前向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([3.0, 1.0], requires_grad=True)\n",
    "b = a * a\n",
    "loss = b.mean()\n",
    "loss2 = loss ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时还没有调用 `.backward()` 方法，这时打印叶子节点的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现为空。现在调用 `.backward()` 方法，这时再次打印叶子节点的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 1.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在叶子节点有了梯度，但非叶子节点的梯度因为被自动清空（节省显存）而仍为空。但如果此时再次调用 `.backward()` 就会报告会 `RuntimeError`。这是这样因为Pytorch是动态计算图机制，一次backward后计算图就清空了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/anaconda3/envs/forward/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/forward/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试在前向传播之后仅替换loss的值，而继续利用已经建立的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss2 grad:  tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202116/859236651.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  a.grad, loss.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([10.,  5.]), None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "b = a ** 2\n",
    "loss = b.mean()\n",
    "loss2 = loss ** 2\n",
    "\n",
    "loss2.register_hook(lambda grad: print('loss2 grad: ', grad))\n",
    "loss2.backward()\n",
    "a.grad, loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss2 grad:  tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([10.,  5.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "b = a ** 2\n",
    "loss = b.mean()\n",
    "loss2 = loss ** 2\n",
    "loss2.data = torch.tensor([3.4], requires_grad=False)\n",
    "loss2.register_hook(lambda grad: print('loss2 grad: ', grad))\n",
    "loss2.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用矩阵乘法举例子\n",
    "w = torch.tensor([[1., 2.],\n",
    "              [0., 2.]], requires_grad=True)\n",
    "b = torch.tensor([1., 1.], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([1., 1.], requires_grad=False)\n",
    "\n",
    "y = torch.tensor([3., 3.]) # y 不显式指定requires_grad，从结果可以知道，默认该项为False\n",
    "\n",
    "y_pred = w @ x + b\n",
    "\n",
    "loss = (y - y_pred).pow(2).mean()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [-0., -0.]]),\n",
       " tensor([1., -0.]),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "w.grad, b.grad, x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [-0., -0.]]),\n",
       " tensor([1., -0.]),\n",
       " tensor([1., 2.]),\n",
       " None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用矩阵乘法举例子\n",
    "w = torch.tensor([[1., 2.],\n",
    "              [0., 2.]], requires_grad=True)\n",
    "b = torch.tensor([1., 1.], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([1., 1.], requires_grad=True)\n",
    "\n",
    "y = torch.tensor([3., 3.]) # y 显式指定requires_grad=True，则数据的梯度也被计算了，但是这不会有太大问题，因为1,数据一般都是False的，2,即使True，由于optimizer并未管理这一部分张量，实际上只是多存储了一部分梯度而已，但不会导致计算错误\n",
    "\n",
    "y_pred = w @ x + b\n",
    "\n",
    "loss = (y - y_pred).pow(2).mean()\n",
    "\n",
    "loss.backward()\n",
    "w.grad, b.grad, x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000],\n",
       "         [0.5000, 0.5000]]),\n",
       " tensor([1.0000, 0.5000]),\n",
       " None,\n",
       " None,\n",
       " tensor(0.6250, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用矩阵乘法举例子\n",
    "w = torch.tensor([[1., 2.],\n",
    "              [0., 2.5]], requires_grad=True)\n",
    "b = torch.tensor([1., 1.], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([1., 1.])\n",
    "\n",
    "y = torch.tensor([3., 3.]) \n",
    "\n",
    "y_pred = w @ x + b\n",
    "\n",
    "loss = (y - y_pred).pow(2).mean()\n",
    "# loss.data = torch.tensor([3.4], requires_grad=False)\n",
    "\n",
    "loss.backward()\n",
    "w.grad, b.grad, x.grad, y.grad, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000],\n",
       "         [0.5000, 0.5000]]),\n",
       " tensor([1.0000, 0.5000]),\n",
       " None,\n",
       " None,\n",
       " tensor([3.4000], grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用矩阵乘法举例子\n",
    "w = torch.tensor([[1., 2.],\n",
    "              [0., 2.5]], requires_grad=True)\n",
    "b = torch.tensor([1., 1.], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([1., 1.])\n",
    "\n",
    "y = torch.tensor([3., 3.]) \n",
    "\n",
    "y_pred = w @ x + b\n",
    "\n",
    "loss = (y - y_pred).pow(2).mean()\n",
    "loss.data = torch.tensor([3.4], requires_grad=False)\n",
    "\n",
    "loss.backward()\n",
    "w.grad, b.grad, x.grad, y.grad, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [-0., -0.]]),\n",
       " tensor([1., -0.]),\n",
       " tensor([1., 2.]),\n",
       " None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用矩阵乘法举例子\n",
    "w = torch.tensor([[1., 2.],\n",
    "              [0., 2.]], requires_grad=True)\n",
    "b = torch.tensor([1., 1.], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([1., 1.], requires_grad=True) # x 显式指定requires_grad=True，则数据的梯度也被计算了，但是这不会有太大问题，因为1,数据一般都是False的，2,即使True，由于optimizer并未管理这一部分张量，实际上只是多存储了一部分梯度而已，但不会导致计算错误\n",
    "\n",
    "y = torch.tensor([3., 3.]) \n",
    "\n",
    "y_pred = w @ x + b\n",
    "\n",
    "loss = (y - y_pred).pow(2).mean()\n",
    "loss.data = torch.tensor([3.4], requires_grad=False)\n",
    "\n",
    "loss.backward()\n",
    "w.grad, b.grad, x.grad, y.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
