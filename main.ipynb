{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC: Forward\n",
    "\n",
    "Mem: Forward\n",
    "\n",
    "用Memristor返回的数值替换PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 47.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.06754004955291748\n",
      "test error: 0.06850004196166992\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
    "\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def overlay_y_on_x(x, y):\n",
    "    \"\"\"Replace the first 10 pixels of data [x] with one-hot-encoded label [y]\n",
    "    \"\"\"\n",
    "    x_ = x.clone()\n",
    "    x_[:, :10] *= 0.0\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
    "\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            h = overlay_y_on_x(x, label)\n",
    "            goodness = []\n",
    "            for layer in self.layers:\n",
    "                h = layer(h)\n",
    "                goodness += [h.pow(2).mean(1)]\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(1)\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print('training layer', i, '...')\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
    "\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=0.03)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 1000\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        return self.relu(\n",
    "            torch.mm(x_direction, self.weight.T) +\n",
    "            self.bias.unsqueeze(0))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        for i in tqdm(range(self.num_epochs)):\n",
    "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
    "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
    "            # The following loss pushes pos (neg) samples to\n",
    "            # values larger (smaller) than the self.threshold.\n",
    "            loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                -g_pos + self.threshold,\n",
    "                g_neg - self.threshold]))).mean()\n",
    "            self.opt.zero_grad()\n",
    "            # this backward just compute the derivative and hence\n",
    "            # is not considered backpropagation.\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
    "\n",
    "    \n",
    "def visualize_sample(data, name='', idx=0):\n",
    "    reshaped = data[idx].cpu().reshape(28, 28)\n",
    "    plt.figure(figsize = (4, 4))\n",
    "    plt.title(name)\n",
    "    plt.imshow(reshaped, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "torch.manual_seed(1234)\n",
    "train_loader, test_loader = MNIST_loaders()\n",
    "\n",
    "net = Net([784, 500, 500])\n",
    "x, y = next(iter(train_loader))\n",
    "x, y = x.cuda(), y.cuda()\n",
    "x_pos = overlay_y_on_x(x, y)\n",
    "rnd = torch.randperm(x.size(0))\n",
    "x_neg = overlay_y_on_x(x, y[rnd])\n",
    "\n",
    "# for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
    "#     visualize_sample(data, name)\n",
    "\n",
    "net.train(x_pos, x_neg)\n",
    "\n",
    "print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(test_loader))\n",
    "x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "\n",
    "print('test error:', 1.0 - net.predict(x_te).eq(y_te).float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Layer(\n",
       "   in_features=784, out_features=500, bias=True\n",
       "   (relu): ReLU()\n",
       " ),\n",
       " Layer(\n",
       "   in_features=500, out_features=500, bias=True\n",
       "   (relu): ReLU()\n",
       " )]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net([784, 500, 500])\n",
    "net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.cuda(), y.cuda()\n",
    "x_pos = overlay_y_on_x(x, y)\n",
    "rnd = torch.randperm(x.size(0))\n",
    "x_neg = overlay_y_on_x(x, y[rnd])\n",
    "\n",
    "net.train(x_pos, x_neg)\n",
    "\n",
    "print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(test_loader))\n",
    "x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "\n",
    "print('test error:', 1.0 - net.predict(x_te).eq(y_te).float().mean().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
