{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epochs: 1], Loss: [0.13042461 0.07806853]\n",
      "Train Accuracy: 90.28666416803995\n",
      "Test Accuracy: 90.62999486923218\n",
      "[Epochs: 2], Loss: [0.03802382 0.02492961]\n",
      "Train Accuracy: 91.96166396141052\n",
      "Test Accuracy: 91.97999835014343\n",
      "[Epochs: 3], Loss: [0.02603848 0.01683783]\n",
      "Train Accuracy: 93.10999810695648\n",
      "Test Accuracy: 92.91999936103821\n",
      "[Epochs: 4], Loss: [0.01848117 0.0123628 ]\n",
      "Train Accuracy: 94.09499764442444\n",
      "Test Accuracy: 93.61000061035156\n",
      "[Epochs: 5], Loss: [nan nan]\n",
      "Train Accuracy: 9.871666505932808\n",
      "Test Accuracy: 9.799999743700027\n",
      "[Epochs: 6], Loss: [nan nan]\n",
      "Train Accuracy: 9.871666381756466\n",
      "Test Accuracy: 9.799999743700027\n",
      "[Epochs: 7], Loss: [nan nan]\n",
      "Train Accuracy: 9.871666381756466\n",
      "Test Accuracy: 9.799999743700027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 257\u001b[0m\n\u001b[1;32m    255\u001b[0m train_loader,val_loader\u001b[38;5;241m=\u001b[39mMNIST_loaders(\u001b[38;5;241m10000\u001b[39m,\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m    256\u001b[0m net \u001b[38;5;241m=\u001b[39m FNet([\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m1000\u001b[39m])\n\u001b[0;32m--> 257\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 213\u001b[0m, in \u001b[0;36mFNet.train\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    209\u001b[0m   h_pos, h_neg \u001b[38;5;241m=\u001b[39m x_pos\u001b[38;5;241m.\u001b[39mcuda(), x_neg\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    211\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m k, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 213\u001b[0m     h_pos, h_neg,loss \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mh_neg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_list[k]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_list[k]\u001b[38;5;241m+\u001b[39mloss\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_list\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdivide(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_list, j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 151\u001b[0m, in \u001b[0;36mLayer.train\u001b[0;34m(self, x_pos, x_neg, k)\u001b[0m\n\u001b[1;32m    149\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 151\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x_pos,k)\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x_neg,k)\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs_internal\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import io\n",
    "import sys\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def F_Mnist(batch_size_train,batch_size_test):\n",
    "  compress_factor = 1\n",
    "  reshape_f = lambda x: torch.reshape(x[0, ::compress_factor, ::compress_factor], (-1, ))\n",
    "\n",
    "  transforms_train = Compose([\n",
    "    #transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "    #transforms.RandomRotation(5),     #Rotates the image to a specified angel\n",
    "    transforms.RandomAffine(0, translate= (.025,.025),shear=0), #Performs actions like zooms, change shear angles.\n",
    "    #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # Set the color params\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),Lambda(lambda x: torch.flatten(x)) ])\n",
    "  transforms_val = transforms.Compose([transforms.ToTensor(),  transforms.Normalize((0.1307,), (0.3081,)),Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "  train_dataset = datasets.FashionMNIST(\"~/.pytorch/F_MNIST_data/\", train=True, download=True, transform=transforms_train)\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "  val_dataset = datasets.FashionMNIST(\"~/.pytorch/F_MNIST_data/\", train=False, download=True, transform=transforms_val)\n",
    "  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "  return train_loader,val_loader\n",
    "\n",
    "\n",
    "def MNIST_loaders(train_batch_size=100, test_batch_size=10000):\n",
    "\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def overlay_y_on_x(x, y):\n",
    "    x_ = x.clone()\n",
    "    x_[:,:10] *= 0.0\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_\n",
    "\n",
    "\n",
    "\n",
    "class CustomCosineSimilarity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCosineSimilarity, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 确保x和y扩充到2D\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        if y.dim() == 1:\n",
    "            y = y.unsqueeze(0)\n",
    "        \n",
    "        # 计算内积，考虑到我们处理的是2D张量，这里使用matmul而非dot！！\n",
    "        inner_product = torch.matmul(x, y.T)\n",
    "\n",
    "        # 用最大值替代模长，避免求导的复杂性\n",
    "        max_x = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        max_y = torch.max(y, dim=1, keepdim=True)[0]\n",
    "\n",
    "        result = inner_product / (max_x * max_y.T + 1e-6)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.P=nn.Linear(in_features, out_features,bias=True).cuda()\n",
    "        self.opt = Adam(self.P.parameters(),weight_decay=0, lr=0.0001)\n",
    "        self.threshold = 4\n",
    "        self.running_loss=0.0\n",
    "        self.num_epochs_internal =25\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.gelu=torch.nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x,k):\n",
    "      x=self.P(x)\n",
    "      return self.gelu(x)\n",
    "\n",
    "\n",
    "\n",
    "    def goodness(self,x_pos,x_neg,k):\n",
    "      # cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "      cos = CustomCosineSimilarity()\n",
    "      if k==0:\n",
    "        g_p = (self.forward(x_pos,k))\n",
    "        g_pos =cos(g_p, p_vector0)\n",
    "\n",
    "        g_n = (self.forward(x_neg,k))\n",
    "        g_neg =cos(g_n, p_vector0)\n",
    "      if k==1:\n",
    "        g_p = (self.forward(x_pos,k))\n",
    "        g_pos =cos(g_p, p_vector1)\n",
    "\n",
    "        g_n = (self.forward(x_neg,k))\n",
    "        g_neg =cos(g_n, p_vector1)\n",
    "\n",
    "      return g_pos,g_neg\n",
    "\n",
    "\n",
    "    def train(self, x_pos, x_neg,k):\n",
    "        self.running_loss=0.0\n",
    "        for i in range(self.num_epochs_internal):\n",
    "\n",
    "          g_pos,g_neg=self.goodness(x_pos,x_neg,k)\n",
    "          delta=g_pos-g_neg\n",
    "          loss = (torch.log(1 + torch.exp(\n",
    "              -self.threshold*delta ))).mean()\n",
    "          self.opt.zero_grad()\n",
    "          loss.backward()\n",
    "          self.opt.step()\n",
    "          self.running_loss+=loss.item()\n",
    "        return self.forward(x_pos,k).detach(), self.forward(x_neg,k).detach(), self.running_loss/self.num_epochs_internal\n",
    "\n",
    "\n",
    "acc_test=[]\n",
    "acc_train=[]\n",
    "\n",
    "class FNet(torch.nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.J=torch.ones(40,40).cuda()\n",
    "        self.e=torch.ones(1,40).cuda()\n",
    "        self.layers = []\n",
    "        self.para_weight=[]\n",
    "        self.para_bias=[]\n",
    "        self.loss_save=defaultdict(list)\n",
    "        self.param_save=[]\n",
    "        self.num_epochs =20\n",
    "        self.num_label=10\n",
    "        self.POOL = nn.AvgPool2d(3, stride=2,padding=1)\n",
    "        self.Pad = nn.ZeroPad2d((0,0,0,4))\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.N_layer=int(len(dims)/2)\n",
    "        self.loss_list=[0] * int(len(dims)/2)\n",
    "        for d in range(0, (len(dims)), 2):\n",
    "            self.layers += [Layer( dims[d ], dims[d+1 ]).cuda()]\n",
    "\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(self.num_label):\n",
    "            h = overlay_y_on_x(x, label)\n",
    "            goodness = []\n",
    "            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            # cos = CustomCosineSimilarity()\n",
    "            for k, layer in enumerate(self.layers):\n",
    "              h = layer(h,k)\n",
    "              g = (h)\n",
    "              if k==0:\n",
    "                goodness += [cos(g, p_vector0)]\n",
    "              if k==1:\n",
    "                goodness += [cos(g, p_vector1)]\n",
    "\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(1)\n",
    "\n",
    "    def train(self, train_loader,val_loader):\n",
    "        k=0\n",
    "        for i in range(self.num_epochs):\n",
    "            self.loss_list=[0] * self.N_layer\n",
    "            for j, data in enumerate(train_loader, 0):\n",
    "              x, y = data\n",
    "              x, y = x.cuda(),y.cuda()\n",
    "              x_pos = overlay_y_on_x(x, y)\n",
    "              y_n=y.clone()\n",
    "              for s in range(x.size(0)):\n",
    "                y_n[s]=int(choice(list(set(range(0, self.num_label)) - set([y[s].item()]))))\n",
    "              x_neg = overlay_y_on_x(x, y_n)\n",
    "              h_pos, h_neg = x_pos.cuda(), x_neg.cuda()\n",
    "\n",
    "              for k, layer in enumerate(self.layers):\n",
    "\n",
    "                h_pos, h_neg,loss = layer.train(h_pos,  h_neg,k)\n",
    "\n",
    "                self.loss_list[k]=self.loss_list[k]+loss\n",
    "\n",
    "\n",
    "            self.loss_list=np.divide(self.loss_list, j+1)\n",
    "\n",
    "\n",
    "            for l in range(self.N_layer):\n",
    "                  self.loss_save[l].append(self.loss_list[l])\n",
    "            print( f'[Epochs: {i + 1}], Loss: {self.loss_list}' )\n",
    "            # if i%10==9:\n",
    "            with torch.no_grad():\n",
    "              er=0.0\n",
    "              for g, data in enumerate(train_loader, 0):\n",
    "                x_t, y_t = data\n",
    "                x_t, y_t = x_t.cuda(), y_t.cuda()\n",
    "                er+=( self.predict(x_t).eq(y_t).float().mean().item())\n",
    "\n",
    "              print('Train Accuracy:', (er)/(g+1)*100)\n",
    "              acc_train.append((er)/(g+1)*100)\n",
    "              er=0.0\n",
    "              for t, data in enumerate(val_loader, 0):\n",
    "                x_te, y_te = data\n",
    "                x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "                er+=( self.predict(x_te).eq(y_te).float().mean().item())\n",
    "              print('Test Accuracy:', (er)/(t+1)*100)\n",
    "              acc_test.append((er)/(t+1)*100)\n",
    "\n",
    "\n",
    "\n",
    "p_vector0=torch.normal(0, 1, size=(1, 1000))\n",
    "p_vector0=p_vector0/ (p_vector0.norm(2, 1, keepdim=True) + 1e-4)\n",
    "p_vector1=torch.normal(0, 1, size=(1, 1000))\n",
    "p_vector1=p_vector1/ (p_vector1.norm(2, 1, keepdim=True) + 1e-4)\n",
    "\n",
    "p_vector0=(p_vector0.repeat(10000, 1)).cuda()\n",
    "p_vector1=(p_vector1.repeat(10000, 1)).cuda()\n",
    "\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "train_loader,val_loader=MNIST_loaders(10000,10000)\n",
    "net = FNet([28*28,1000,1000,1000])\n",
    "net.train(train_loader,val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vector0=torch.normal(0, 1, size=(1, 1000))\n",
    "p_vector0=p_vector0/ (p_vector0.norm(2, 1, keepdim=True) + 1e-4)\n",
    "p_vector1=torch.normal(0, 1, size=(1, 1000))\n",
    "p_vector1=p_vector1/ (p_vector1.norm(2, 1, keepdim=True) + 1e-4)\n",
    "\n",
    "p_vector0=(p_vector0.repeat(10000, 1)).cuda()\n",
    "p_vector1=(p_vector1.repeat(10000, 1)).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vector0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.5906e-01, -1.2064e+00,  3.5593e-01,  8.9577e-01,  1.2560e+00,\n",
       "           1.4983e+00,  2.3623e-02, -1.9295e-01,  5.7306e-03,  7.7744e-01,\n",
       "           6.2530e-01, -1.6878e+00,  8.9308e-01, -1.6010e+00,  1.8778e+00,\n",
       "          -2.7001e-01,  2.2697e-01, -5.8961e-01,  1.4445e+00, -2.7240e-01,\n",
       "          -1.6261e+00, -1.2220e+00,  1.0766e-01, -8.3358e-01, -1.0103e+00,\n",
       "          -1.6387e+00,  1.6424e+00,  1.2925e+00,  7.4663e-01, -1.6416e-02,\n",
       "          -1.9127e-01, -1.8558e+00,  1.8371e-01,  6.2509e-01,  2.2333e-01,\n",
       "          -1.1747e+00, -2.1887e+00, -1.5026e-01,  1.3098e+00, -5.3937e-01,\n",
       "          -1.0065e+00,  4.8031e-01,  3.8939e-01,  1.8480e+00,  8.4694e-01,\n",
       "           1.1861e+00, -1.3060e-01,  1.4472e+00,  5.3964e-01,  3.8300e-01,\n",
       "          -7.0488e-01, -4.2418e-01,  4.8299e-01,  8.8429e-01,  4.0133e-01,\n",
       "           2.1059e-01, -6.9364e-01,  3.0224e-02,  3.3514e-01,  9.0764e-02,\n",
       "           5.6317e-01, -7.1562e-01,  8.3180e-01, -1.6389e+00,  7.9661e-01,\n",
       "          -3.5473e-02,  7.2875e-01,  8.8055e-01, -1.2522e+00,  7.7647e-01,\n",
       "           1.5411e+00, -2.5630e-01,  1.9231e+00, -2.5376e-02, -1.2043e+00,\n",
       "           3.4789e-01,  9.1198e-01,  3.6273e-01, -2.2266e+00, -3.4232e-01,\n",
       "          -7.6852e-03,  3.7636e-01,  1.8616e+00, -5.5791e-01,  1.4066e+00,\n",
       "           1.1416e+00, -1.5185e-01, -1.5072e-01, -1.8827e-01, -1.2719e-01,\n",
       "          -4.4609e-01,  6.5250e-01,  1.7813e+00, -1.4420e+00,  5.6302e-01,\n",
       "          -5.4431e-01, -1.3070e+00, -3.2099e-01, -1.1468e+00,  5.8268e-01,\n",
       "           6.8307e-01,  1.3810e+00, -3.0125e-01,  1.1774e+00,  1.1542e-01,\n",
       "           1.9466e+00,  1.0463e+00, -4.6641e-01,  1.0820e+00,  3.0496e-02,\n",
       "           4.2685e-01, -4.5392e-01,  5.2119e-01, -8.6498e-01,  9.7530e-01,\n",
       "          -8.6945e-02,  3.6475e-01, -4.6758e-01, -3.4419e-01, -7.3384e-01,\n",
       "           1.5604e+00,  6.3438e-01,  1.6324e+00,  1.1386e+00, -1.6376e-01,\n",
       "          -1.5481e+00,  1.4575e+00, -9.1494e-01, -3.1340e-01,  7.3442e-01,\n",
       "           1.4910e-01, -1.8133e+00, -6.5825e-01, -6.5450e-01, -4.8391e-01,\n",
       "          -1.0351e+00,  1.1063e-01, -1.4157e+00,  9.6536e-01, -5.7719e-01,\n",
       "          -3.5457e-01,  1.3430e+00,  1.2371e-01, -1.0715e-01, -7.6672e-01,\n",
       "           1.1686e+00,  1.1541e+00, -9.2078e-01,  9.1770e-01, -2.6271e-01,\n",
       "           1.5944e+00, -1.5489e-01,  7.9775e-01,  5.7701e-01, -7.3033e-01,\n",
       "           7.4517e-01, -1.2438e+00,  7.3691e-01, -1.0658e+00,  6.1241e-01,\n",
       "           8.1128e-02,  6.9950e-01, -1.8145e+00,  6.4879e-03, -1.6250e-01,\n",
       "          -1.7771e+00,  1.5357e+00,  5.7549e-01,  4.1729e-01, -1.0516e+00,\n",
       "           1.9319e-01, -1.1562e+00,  2.8688e-01, -6.1959e-01, -2.6410e+00,\n",
       "           9.2653e-01, -7.7138e-01,  8.2397e-01,  4.7003e-02, -6.1384e-02,\n",
       "           6.6722e-01, -1.1935e+00,  8.2911e-01,  2.1144e-01, -8.0173e-01,\n",
       "          -1.1973e+00,  1.0205e+00,  4.8805e-01, -1.0692e+00,  2.9872e-01,\n",
       "          -1.3182e+00, -1.9238e-01, -2.7062e-01,  2.2949e-01, -1.5420e+00,\n",
       "           8.5788e-01, -3.3059e-01, -3.8778e-01, -3.7320e-01,  1.9626e-01,\n",
       "           1.7778e-01,  1.3177e+00, -6.2520e-01,  5.6277e-01, -1.6603e-01,\n",
       "           1.1734e+00, -9.1783e-01,  7.1974e-01,  2.2458e+00, -5.8971e-02,\n",
       "          -5.1420e-01, -1.4791e+00,  6.1721e-01,  7.2038e-01,  3.6286e-01,\n",
       "           1.0136e+00, -3.8876e-01, -1.2560e+00, -1.2191e+00, -3.1640e-01,\n",
       "          -1.7019e-01, -8.8509e-02, -9.4628e-01, -5.0938e-02, -6.1770e-01,\n",
       "           2.8455e-01, -4.6666e-01, -5.1642e-01, -7.1630e-02, -3.7498e-01,\n",
       "           1.2723e+00,  3.0558e-01,  9.8685e-01,  1.3704e-01, -2.2755e-01,\n",
       "           4.5862e-02, -1.7560e+00,  3.0122e-01, -1.6040e+00,  3.9908e-01,\n",
       "          -1.7304e+00, -1.4061e+00,  5.9855e-01,  3.8657e-01,  1.0606e+00,\n",
       "          -7.0503e-01,  1.6464e+00, -1.6600e+00, -1.8094e+00,  2.0685e-01,\n",
       "           2.1209e+00, -1.0935e+00,  1.4074e+00,  5.5384e-01,  3.2477e-01,\n",
       "          -5.5231e-01,  1.8217e+00,  2.8712e-01, -5.8825e-01, -9.6976e-02,\n",
       "           1.0414e+00,  8.7818e-01,  5.7032e-01,  1.0176e+00, -7.3560e-01,\n",
       "          -2.5145e-02,  2.2220e+00, -7.7065e-01,  2.3582e-01, -1.0176e+00,\n",
       "           1.0805e+00, -5.0914e-01,  1.0384e+00,  1.0951e+00, -3.7507e-01,\n",
       "           2.8015e-02,  1.9660e+00, -2.8258e-01, -8.3933e-01, -3.6545e-01,\n",
       "          -1.0333e+00, -1.7054e+00,  1.9099e+00, -8.4648e-01,  2.1289e-01,\n",
       "           1.6516e+00,  1.1890e+00, -8.7036e-02,  1.6885e-01,  8.5987e-01,\n",
       "          -2.4533e+00, -7.1265e-01, -2.0340e+00, -1.4981e+00, -1.2236e+00,\n",
       "           1.0056e+00,  1.7991e+00, -2.0264e+00,  4.1591e-01, -7.1823e-01,\n",
       "           3.8078e-01,  9.5628e-01, -1.4049e+00,  1.5858e+00,  9.3018e-01,\n",
       "          -1.4731e-02,  2.8961e-01, -1.0622e+00, -1.1605e-01, -1.1445e+00,\n",
       "           8.5804e-01, -1.0731e-01,  3.9971e-01,  3.6450e-01, -4.8952e-01,\n",
       "          -1.1168e+00,  5.4627e-02,  1.4712e+00, -1.2400e+00,  1.4051e+00,\n",
       "          -1.3932e+00, -4.6859e-01,  1.3083e+00, -1.2440e+00, -1.7273e-02,\n",
       "           8.6476e-01, -1.0629e+00, -3.6602e-02,  1.8929e+00, -5.0815e-01,\n",
       "           9.0471e-01, -1.1367e+00,  1.1503e+00,  4.0895e-02,  3.6714e-01,\n",
       "          -4.9786e-01,  3.6952e-01, -7.9521e-01,  4.1158e-01, -4.8237e-01,\n",
       "           1.4760e+00, -7.6239e-01, -7.4831e-01, -2.0618e+00,  1.0184e-01,\n",
       "           5.8922e-01,  4.2521e-01,  4.3189e-01,  1.0462e+00, -7.7730e-01,\n",
       "           1.9209e+00, -6.0323e-01,  3.0711e-01, -7.5762e-01, -3.4920e-01,\n",
       "          -7.9422e-01, -2.5370e-01,  7.4231e-01,  6.3279e-01,  7.8446e-01,\n",
       "           1.0597e+00,  7.8042e-01, -1.7417e+00,  7.2585e-01, -1.4035e+00,\n",
       "          -6.9067e-01,  4.0525e-01,  1.1772e+00,  3.3379e-01, -7.6472e-01,\n",
       "           1.3726e-01, -1.2095e+00,  6.4635e-01,  1.0028e+00, -1.2867e+00,\n",
       "           5.3734e-01, -9.6119e-01,  5.3447e-01,  3.3281e-01, -2.8165e-01,\n",
       "           6.0161e-01, -5.3142e-01, -1.1298e+00,  4.2396e-01, -1.4586e+00,\n",
       "          -9.2112e-01, -3.1281e-01, -7.3044e-01,  3.3416e-01,  1.8482e-01,\n",
       "           1.3008e+00,  9.8174e-01,  3.6702e-01, -2.1449e+00,  6.8841e-02,\n",
       "           4.3989e-01,  1.5497e+00,  1.2384e+00,  2.3361e+00, -1.1808e+00,\n",
       "           7.4506e-01,  3.4068e-01, -2.3602e-01, -2.0802e-01, -2.7862e-01,\n",
       "          -5.6557e-01,  2.7711e-01,  6.6914e-01, -2.5296e-01, -6.7327e-01,\n",
       "           1.6628e+00, -1.3349e+00, -8.6725e-01,  7.9327e-01, -1.8620e+00,\n",
       "          -1.2239e+00,  2.1304e-01,  1.6358e-01,  6.2704e-01, -1.5930e+00,\n",
       "           8.7645e-01,  1.3808e+00, -9.2238e-01, -6.7524e-01, -4.8620e-01,\n",
       "          -8.0068e-01, -4.8752e-02, -9.2567e-01, -1.4012e+00,  5.3175e-01,\n",
       "           6.7958e-01, -1.6933e-01, -2.2458e+00, -1.0132e+00,  2.0378e+00,\n",
       "           4.4625e-01,  5.6474e-01, -8.8637e-01,  6.0749e-02, -1.0210e-01,\n",
       "          -4.8322e-01,  8.1563e-01,  9.2309e-01,  3.9004e-01,  1.1719e+00,\n",
       "           1.1532e+00, -1.5972e-01,  8.7092e-01,  2.8636e-01,  7.9406e-01,\n",
       "           1.1641e+00,  1.9957e+00, -1.8735e+00, -8.6837e-01, -2.8616e-01,\n",
       "           7.3131e-01, -1.5371e+00,  1.4028e+00,  4.2806e-01, -9.2998e-01,\n",
       "          -7.5279e-01, -1.0527e+00, -5.4849e-01,  1.0822e+00,  6.3732e-01,\n",
       "          -2.4472e+00, -1.1605e+00, -6.2930e-01,  1.7223e+00,  1.1629e+00,\n",
       "          -1.4837e-01, -1.2087e+00,  1.5958e+00,  1.4458e+00, -2.7764e-01,\n",
       "           9.1792e-01,  2.3830e-01,  4.4555e-01,  6.5048e-01, -1.7880e+00,\n",
       "          -2.5191e-01, -6.3783e-01,  8.4789e-01,  1.0492e+00,  8.4842e-02,\n",
       "          -1.2566e+00, -4.7603e-02,  4.9506e-01, -2.2837e-01,  3.3194e-01,\n",
       "           1.2316e-01, -7.4071e-01,  3.6822e-01,  7.5852e-01, -1.2982e+00,\n",
       "          -5.4324e-01, -1.9105e+00, -6.0551e-01,  3.7047e-02, -1.0526e-01,\n",
       "           4.1520e-01,  1.4738e-02, -6.2105e-01,  8.5188e-01, -1.8027e+00,\n",
       "           1.9226e+00, -7.0194e-01, -4.4601e-02, -1.4030e-01,  4.3384e-01,\n",
       "          -1.6838e-01,  1.3281e+00,  2.1937e-02, -1.4606e+00,  2.4111e-01,\n",
       "           1.0767e-03, -8.0069e-01,  2.0115e-01, -1.9251e+00, -5.8163e-01,\n",
       "           1.8209e-01,  2.6557e-01, -9.8816e-01, -1.9201e+00, -1.5701e+00,\n",
       "           1.6235e-01,  1.1705e+00, -3.8605e-01, -7.2125e-01, -2.5226e-01,\n",
       "          -9.9298e-02, -1.2197e+00, -3.5180e-01, -1.7813e+00, -1.4056e+00,\n",
       "          -1.5297e+00,  9.5612e-01,  3.0373e+00,  8.4537e-02, -7.3081e-01,\n",
       "           1.4835e+00,  6.3525e-01,  2.0990e-01, -2.4164e-01,  1.3775e-01,\n",
       "           8.9024e-02, -1.7467e+00, -6.4000e-01, -1.5045e+00,  5.3877e-01,\n",
       "           4.7073e-01, -9.5313e-02,  6.3554e-01,  1.9026e+00, -5.8723e-02,\n",
       "          -6.5123e-01, -4.6083e-01,  1.6824e-01,  4.3593e-01, -7.1979e-01,\n",
       "          -1.4282e+00, -1.5952e-01, -3.9985e-01,  2.7493e+00, -1.3154e+00,\n",
       "          -4.6630e-01, -8.3448e-01,  2.1562e-01, -1.5522e+00,  1.5826e-01,\n",
       "           1.0281e+00, -1.2220e+00, -4.3132e-01,  4.8005e-01,  7.1759e-01,\n",
       "           3.1884e-01,  8.9985e-01, -8.2853e-01,  5.2395e-01,  1.1686e+00,\n",
       "           6.4440e-01, -1.2430e+00,  2.1303e+00, -4.4192e-02,  1.3088e+00,\n",
       "           9.5000e-01,  3.2929e-01,  1.7330e-01, -7.7776e-03, -1.3672e+00,\n",
       "          -3.9235e-01,  2.1155e+00, -5.0148e-02,  9.0723e-02, -1.1648e+00,\n",
       "          -5.8854e-01,  5.0073e-01, -1.0878e-01,  5.0654e-01,  7.7583e-01,\n",
       "          -4.1242e-01,  4.4036e-01,  1.1701e+00,  2.6135e-01,  5.0325e-02,\n",
       "          -8.0013e-01,  5.4864e-02, -4.1707e-03,  9.1584e-01, -5.4584e-01,\n",
       "           7.4609e-01, -1.8133e+00,  6.3870e-01, -1.8359e+00, -2.8914e-01,\n",
       "           1.4777e+00,  8.9091e-01,  5.9024e-01, -2.8485e-01,  2.4525e-02,\n",
       "           1.2892e+00, -1.3900e+00,  1.0706e+00,  1.1875e+00, -8.1532e-01,\n",
       "           1.1783e+00,  8.3625e-01,  9.9014e-01,  8.4209e-02, -1.7558e-01,\n",
       "           7.5536e-01, -1.4678e+00, -4.3511e-01, -1.0146e+00,  1.1221e+00,\n",
       "          -6.5791e-01, -1.2695e+00,  1.4464e+00, -3.7786e-01, -4.7399e-02,\n",
       "          -1.2190e+00, -8.0206e-01, -2.5378e-01, -1.0168e+00, -9.2926e-01,\n",
       "           3.4088e-01, -8.9574e-01,  2.3493e+00,  1.5417e+00, -1.0076e+00,\n",
       "           1.0829e-01,  1.7087e+00, -8.9772e-01, -2.9602e+00, -2.0090e+00,\n",
       "           1.7440e+00, -3.0089e+00, -1.8592e-01,  5.4005e-01,  1.7491e+00,\n",
       "           5.2692e-01, -4.0662e-01, -5.3147e-01, -6.0118e-01,  2.5438e+00,\n",
       "          -1.6756e+00, -2.2357e+00,  1.9983e+00,  1.1415e-01,  1.5497e+00,\n",
       "           6.5824e-01,  8.1469e-01, -7.9975e-01, -1.5436e+00, -8.9376e-01,\n",
       "           3.3996e-01, -3.3708e-01, -3.2169e-01, -9.7509e-02, -3.2604e-01,\n",
       "          -3.2763e-02, -1.4974e+00, -1.7812e-01,  7.8628e-02, -1.8451e-01,\n",
       "           4.9662e-01,  4.8848e-01, -9.9436e-02,  2.5958e+00, -1.4573e+00,\n",
       "          -5.5388e-01, -1.1941e-02,  5.6284e-01,  1.1510e+00,  1.0519e-01,\n",
       "           1.5996e+00, -2.4990e-02,  2.0311e-01, -8.3370e-01,  1.5580e+00,\n",
       "           2.4801e+00,  1.0496e+00,  1.1236e+00, -1.0592e+00,  3.9195e-01,\n",
       "          -4.2895e-01,  5.7970e-01,  6.1263e-01,  1.8652e+00,  2.4439e-02,\n",
       "          -6.3532e-02,  1.2406e+00,  9.6260e-01,  7.6242e-01,  4.7597e-01,\n",
       "           3.3429e-01, -5.7026e-01,  1.2073e+00, -6.7869e-01, -1.1209e+00,\n",
       "           2.7851e+00, -1.0313e+00,  3.7840e-01,  4.2364e-02,  3.3986e-02,\n",
       "           1.5529e+00, -5.6677e-01,  2.4920e-01, -3.8990e-01,  6.3809e-01,\n",
       "          -1.6231e+00,  6.2519e-01, -7.9704e-01,  7.6047e-01, -3.3155e-01,\n",
       "          -2.8383e-01,  6.1847e-01,  6.0136e-01,  1.4373e+00,  1.6465e+00,\n",
       "           3.5376e-01,  3.7201e-01, -6.9216e-02,  1.5843e-01,  1.4834e-01,\n",
       "           4.9460e-01,  5.9481e-01,  1.0125e+00,  1.9072e+00, -4.6527e-01,\n",
       "           1.6543e+00,  1.0391e+00,  2.3295e-01,  5.5455e-01,  1.5637e+00,\n",
       "           1.1123e+00, -5.0235e-01, -3.3896e-01, -4.0609e-01,  7.5033e-01,\n",
       "          -1.8338e+00,  9.9270e-01, -2.3297e-01, -5.6696e-01, -7.7488e-01,\n",
       "           2.7490e-01,  8.5482e-02, -7.7388e-01,  1.5802e-01,  1.3635e+00,\n",
       "          -8.8285e-01,  9.8444e-01, -4.3161e-01,  1.1610e+00,  7.1192e-01,\n",
       "           5.7381e-01, -1.9067e-01,  7.4474e-01,  4.6404e-01,  6.5880e-01,\n",
       "          -8.3063e-01,  8.3493e-01, -1.2960e+00, -6.8497e-01, -1.4926e+00,\n",
       "           1.3532e+00,  1.1314e-01,  2.1644e-01,  2.2617e+00, -1.5571e+00,\n",
       "          -9.1988e-01,  3.9514e-02,  1.2993e+00, -1.6391e-01,  2.5920e-01,\n",
       "          -2.3440e+00,  1.5723e+00,  4.2267e-01, -1.4213e+00,  6.8321e-01,\n",
       "           6.9376e-02,  1.1177e+00,  1.6416e+00, -4.1448e-01,  7.0015e-01,\n",
       "          -9.4408e-02,  5.1681e-01,  9.8041e-01,  1.4670e-01, -4.1612e-01,\n",
       "          -7.3273e-01,  1.9569e-01, -4.0242e-01,  1.0829e+00,  8.1312e-02,\n",
       "           4.0673e-01, -7.4923e-01,  1.6550e+00, -1.7056e+00,  1.1316e-01,\n",
       "          -2.5890e-01,  2.7798e+00, -7.5534e-01, -4.0529e-01,  7.9723e-01,\n",
       "          -8.9300e-02, -1.3537e+00, -2.2779e+00,  5.5935e-01, -4.7731e-01,\n",
       "          -5.8440e-01, -1.1400e+00, -5.8799e-01, -2.2506e-02, -9.9607e-01,\n",
       "           7.1227e-01,  6.0732e-02,  6.2594e-01, -2.1664e+00,  2.8274e-01,\n",
       "           1.1231e+00, -9.1569e-01, -5.5508e-01,  7.2917e-01,  6.3471e-01,\n",
       "          -1.3337e+00, -1.8236e-01,  2.6924e+00,  3.2489e-02, -8.0500e-01,\n",
       "          -1.3170e+00, -7.6788e-01, -8.8438e-01, -6.7381e-01, -3.3665e-01,\n",
       "           1.7799e-01,  1.5420e+00, -1.2150e+00, -3.1681e-01,  1.0163e+00,\n",
       "          -7.7493e-01, -2.6084e+00, -1.6024e-01, -3.5214e-01,  4.0037e-01,\n",
       "           2.0928e-01, -5.5980e-01,  9.3151e-01, -1.6043e+00,  5.1056e-01,\n",
       "           3.4222e-01,  3.7747e-01, -1.8179e-01,  1.8940e+00,  3.8900e-01,\n",
       "           1.2337e+00, -5.9169e-01,  5.0800e-01,  2.2113e+00,  8.4244e-01,\n",
       "          -2.3439e+00,  1.1681e+00, -1.1486e-01, -4.9026e-01, -1.6811e-02,\n",
       "          -1.4969e-01, -2.0043e-01,  6.4529e-01, -1.1410e-01,  9.4898e-01,\n",
       "           6.8555e-01,  1.4782e+00, -1.1239e-02, -2.2216e+00,  1.8883e-01,\n",
       "           3.4969e-01, -7.0074e-01, -2.2562e-01,  5.8383e-01, -5.8922e-01,\n",
       "           1.1641e+00, -7.1100e-01, -1.5349e+00, -3.8462e-01, -8.0868e-01,\n",
       "          -1.1598e-01, -1.4553e+00, -9.7023e-01, -1.4836e-01,  1.1672e+00,\n",
       "          -3.2274e-02,  5.3483e-01,  4.4993e-01,  1.2693e-01,  1.0015e+00,\n",
       "           3.5766e-01, -1.6942e-02, -2.9928e-03, -1.0140e-02, -1.8839e+00,\n",
       "           8.0580e-01, -1.0866e+00, -1.6241e+00, -1.5434e-02,  1.1554e+00,\n",
       "           4.6850e-01, -6.1838e-01,  2.7767e-01,  3.7204e-01,  7.1649e-01,\n",
       "           1.1126e+00,  1.3667e-01,  9.3134e-01, -1.1299e+00,  2.3350e-02,\n",
       "           1.7853e+00,  4.6596e-01, -2.0684e+00,  8.9283e-01,  1.7808e+00,\n",
       "          -3.0521e-01, -1.4828e+00,  2.2740e-01, -5.1193e-01,  1.7675e+00,\n",
       "          -8.1054e-01, -2.4807e-01, -1.0770e+00, -1.1123e+00, -1.6180e-01,\n",
       "           3.9754e-01,  1.9881e+00,  4.7642e-01,  3.5988e-01,  2.9247e+00,\n",
       "          -3.0888e-01,  1.7546e+00, -2.4289e+00, -6.5636e-01,  7.7445e-02,\n",
       "          -1.1190e+00, -2.3444e+00,  8.3050e-01, -1.2536e+00,  5.7579e-01,\n",
       "          -7.8464e-02, -9.9733e-01, -3.8680e-01, -1.8859e-01,  1.0396e-01,\n",
       "           9.3922e-01,  1.2282e+00, -1.9944e+00,  1.3702e+00, -1.1767e-01,\n",
       "           3.0761e-01,  1.1928e+00,  6.0490e-01, -2.7969e+00,  3.6912e-01,\n",
       "          -2.3339e+00,  5.2972e-02, -1.1061e+00,  7.6467e-01, -3.3299e-01,\n",
       "          -1.6990e+00, -2.1667e+00, -1.3131e+00,  1.7105e+00, -1.3976e+00,\n",
       "          -4.8965e-01, -6.8577e-01,  6.3690e-01, -5.4019e-01,  5.0086e-01,\n",
       "           3.8030e-01,  1.7138e+00, -1.4984e+00,  3.8774e-01, -1.2221e+00]]),\n",
       " torch.Size([1, 1000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0, 1, size=(1, 1000)), torch.normal(0, 1, size=(1, 1000)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
