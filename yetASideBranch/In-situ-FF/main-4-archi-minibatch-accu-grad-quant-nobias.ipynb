{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [21:24<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [22:01<00:00,  7.57it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 250\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# x, y = next(iter(train_loader))\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# # max pooling for x\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# x = x.view(-1, 28, 28)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# # for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# #     visualize_sample(data, name)\u001b[39;00m\n\u001b[1;32m    248\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain(li_epochs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m10000\u001b[39m], li_lr\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m8e-3\u001b[39m, \u001b[38;5;241m8e-3\u001b[39m])\u001b[38;5;66;03m#x_pos, x_neg)\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain accu:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m net\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39meq(y)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    252\u001b[0m x_te, y_te \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# max pooling for x_te\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from train_utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "batchsize = 100\n",
    "#================================\n",
    "# Quantization levels\n",
    "#================================\n",
    "img_half_level = 4\n",
    "weight_bit = 8 \n",
    "output_bit = 6\n",
    "isint = 0\n",
    "clamp_std = 0\n",
    "noise_scale = 5e-2\n",
    "\n",
    "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
    "\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def overlay_y_on_x(x, y):\n",
    "    \"\"\"\n",
    "    Replace the first 10 pixels of data [x] with one-hot-encoded label [y]\n",
    "    \"\"\"\n",
    "    x_ = x.clone()\n",
    "    x_[:, :10] *= 0.0\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
    "\n",
    "    def predict(self, x): \n",
    "        # 这个predict方法是理解ff方法的关键，它不是像普通的predict方法一样，输入一个样本，输出一个长度为num_cls的softmax预测向量\n",
    "        # 而是一个样本反复输入这个网络num_cls次，把每种带标签的可能都计算一个goodness，也就是这个数据是好数据的可能性，找出最高goodness的就是预测类别\n",
    "        goodness_per_label = []\n",
    "        for label in range(10): # 对每一个标签进行预测\n",
    "            h = overlay_y_on_x(x, label) # h是输入x和标签label的叠加\n",
    "            goodness = [] # goodness是一个列表，里面存放了每一层的结果向量的均方\n",
    "            for layer in self.layers: # 对每一层进行前传\n",
    "                h = layer(h) # h是每一层的输出\n",
    "                goodness += [h.pow(2).mean(1)] # goodness是每一层的结果向量的均方。h.pow(2)是h的每一个元素的平方，mean(1)是对每一行求均值\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)] # goodness_per_label是每一层的结果向量的均方的和\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1) # goodness_per_label是每一层的结果向量的均方的和的列表\n",
    "        return goodness_per_label.argmax(1) # 返回的是goodness_per_label中每一行最大值的索引，也就是说，返回的是每一行最大值的列索引\n",
    "\n",
    "    def train(self, li_epochs, li_lr): #, x_pos, x_neg): # 这个train方法是对整个网络进行训练，训练的目标是让正样本的结果向量的均方上升，负样本的结果向量的均方下降\n",
    "        x, y = next(iter(train_loader))\n",
    "        # x, _ = my.data_quantization_sym(x, half_level=img_half_level)\n",
    "        # max pooling for x\n",
    "        x = x.view(-1, 28, 28)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 14*14)\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        x_pos = overlay_y_on_x(x, y)\n",
    "        # rnd = torch.randperm(x.size(0)) # 生成一个从0到n-1的随机整数序列。\n",
    "        # x_neg = overlay_y_on_x(x, y[rnd])\n",
    "        y_rnd = y.clone()\n",
    "        for i, y_i in enumerate(y):\n",
    "            li = list(range(10))\n",
    "            li.remove(y_i)\n",
    "            j = np.random.choice(li)\n",
    "            y_rnd[i] = j\n",
    "\n",
    "        x_neg = overlay_y_on_x(x, y_rnd)\n",
    "\n",
    "        h_pos, h_neg = x_pos, x_neg # h_pos和h_neg是正样本和负样本的输入\n",
    "        for i, layer in enumerate(self.layers): # 对每一层进行训练\n",
    "            print('training layer', i, '...') # 这里的i是层数\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg, num_epochs=li_epochs[i], lr=li_lr[i]) # 对每一层进行训练，得到了正样本和负样本的结果向量，这个结果向量是该层的输出，也是下一层的输入\n",
    "            # 也就是说，这个训练的过程中，正样本在前传过程中得到的每一层输出都被认为是正的，负样本在前传过程中得到的每一层输出都被认为是负的，也就是说，出身决定一切\n",
    "\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=8e-3)\n",
    "        self.threshold = 2.0\n",
    "        # self.num_epochs = num_epochs # 训练的次数是1000次\n",
    "        # self.linear = my.Linear_quant_noise(in_features, out_features, weight_bit=weight_bit, output_bit=output_bit, isint=isint, clamp_std=clamp_std, noise_scale=noise_scale, bias=True)\n",
    "        self.weight_bit = weight_bit\n",
    "        self.output_bit = output_bit\n",
    "        self.isint = 0\n",
    "        self.clamp_std = 0\n",
    "        self.noise_scale = 0\n",
    "        self.weight_half_level = 2 ** weight_bit / 2 - 1\n",
    "        self.output_half_level = 2 ** output_bit / 2 - 1\n",
    "        # self.linear = nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "        self.lossli = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape\", x.shape)\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)  # 这个是对输入做了归一化，使得输入的模长为1，这在论文里有解释\n",
    "        # print(\"x_norm.shape\", x_direction.shape)\n",
    "        # x_direction = x / x.max()\n",
    "        self.weight_, self.bias_ = my.Weight_Quant_Noise.apply(self.weight, self.bias,\n",
    "                                            self.weight_half_level, \n",
    "                                            self.isint, self.clamp_std,\n",
    "                                            self.noise_scale\n",
    "                                            )\n",
    "        self.weight_, self.bias_ = self.weight, self.bias\n",
    "        x_direction = self.relu(\n",
    "            torch.mm(x_direction, self.weight.T) \n",
    "            # + self.bias.unsqueeze(0) # 这个是对输入做了最基本的前向传播，得到了结果向量4\n",
    "            # self.linear(x_direction)\n",
    "            # F.linear(x_direction, self.weight_, self.bias_)\n",
    "            ) # 注意，在前传之后，随即使用了relu激活函数，这意味着每一层的所有激活值都是非负的\n",
    "        \n",
    "        # x = my.Feature_Quant.apply(x, self.output_half_level, self.isint)\n",
    "        # print(\"x_direction.shape\", x_direction.shape)\n",
    "\n",
    "        return x_direction\n",
    "\n",
    "    def train(self, x_pos, x_neg, num_epochs=4000000, lr=8e-3):\n",
    "        opt = Adam(self.parameters(), lr=lr)\n",
    "        # 训练其实就是对每一层分别进行训练，训练的目标是让正样本的结果向量的均方上升，负样本的结果向量的均方下降\n",
    "        # 每一层的forward方法定义如上一个函数，这里的train方法定义了训练的过程\n",
    "        # print(\"x_pos.shape\", x_pos.shape)\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            # minibatch\n",
    "            bz = 100\n",
    "            for j in range(0, x_pos.size(0), bz):\n",
    "                \n",
    "                # print(j)\n",
    "                # mask = torch.randperm(x_pos.size(0))[j: j+bz]\n",
    "                # mask = [j: j+bz]\n",
    "                h_pos = x_pos[j: j+bz] # 随机采样1000个正样本\n",
    "                h_neg = x_neg[j: j+bz] # 随机采样1000个负样本\n",
    "\n",
    "\n",
    "                # for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
    "                #     visualize_sample(data, name)\n",
    "                \n",
    "                # print(self.forward(x_pos).pow(2), self.forward(x_pos).pow(2).shape)\n",
    "                g_pos = self.forward(h_pos).pow(2).mean(1) # g_pos 是正样本x_pos在该层前向传播得到的结果向量的均方\n",
    "                g_neg = self.forward(h_neg).pow(2).mean(1) # g_neg 是负样本x_neg在该层前向传播得到的结果向量的均方\n",
    "                # 论文关于使用L2范数来度量的理由：\n",
    "                # There are two main reasons for using the squared length of the activity vector as the goodness function.\n",
    "                # First, it has very simple derivatives. Second, layer normalization removes all trace of the goodness.\n",
    "                \n",
    "                # The following loss pushes pos (neg) samples to\n",
    "                # values larger (smaller) than the self.threshold.\n",
    "                # 随着训练过程，loss下降，g_pos将上升，g_neg将下降\n",
    "                loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                    -g_pos + self.threshold,\n",
    "                    g_neg - self.threshold]))).mean() # loss = [log(1+exp(-(g_pos-threshold))) + log(1+exp(g_neg-threshold))] / 2\n",
    "                # print(loss)\n",
    "                self.lossli.append(loss.item())  # save loss\n",
    "                \n",
    "                # this backward just compute the derivative and hence\n",
    "                # is not considered backpropagation.\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                # self.opt.step()\n",
    "                # self.opt.zero_grad()\n",
    "                    \n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "            # if i<4:\n",
    "            #     # print(a)\n",
    "            #     # 查看累积的梯度值\n",
    "            #     # print(self.parameters())\n",
    "            #     for param in self.parameters():\n",
    "            #         print(param.grad)\n",
    "                # print(loss)\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # 关于这里为什么能够work：\n",
    "            # 1. loss是权重的函数，loss的核心思想是让g_pos上升，g_neg下降\n",
    "            # 2. g_pos和g_neg是x_pos和x_neg的函数，x_pos和x_neg反映了客观世界，是这样要学习的对象。有了x_pos和x_neg，就能够计算出g_pos和g_neg，有了g_pos和g_neg，就能够计算出loss\n",
    "            # 3. 通过loss.backward()，计算loss对权重的梯度，使得loss下降，g_pos上升，g_neg下降\n",
    "            # 4. 通过self.opt.step()，更新了self.weight和self.bias\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
    "\n",
    "    \n",
    "# def visualize_sample(data, name='', idx=0):\n",
    "#     reshaped = data[idx].cpu().reshape(28, 28)\n",
    "#     plt.figure(figsize = (4, 4))\n",
    "#     plt.title(name)\n",
    "#     plt.imshow(reshaped, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(1234)\n",
    "    train_loader, test_loader = MNIST_loaders(train_batch_size=50000, test_batch_size=10000)\n",
    "\n",
    "    net = Net([784//4, 128, 128])\n",
    "    x, y = next(iter(train_loader))\n",
    "    # max pooling for x\n",
    "    x = x.view(-1, 28, 28)\n",
    "    x = F.max_pool2d(x, 2, 2)\n",
    "    x = x.view(-1, 14*14)\n",
    "\n",
    "    # x, y = x.cuda(), y.cuda()\n",
    "    # x_pos = overlay_y_on_x(x, y)\n",
    "    # # rnd = torch.randperm(x.size(0)) # 生成一个从0到n-1的随机整数序列。\n",
    "    # # x_neg = overlay_y_on_x(x, y[rnd])\n",
    "    # y_rnd = y.clone()\n",
    "    # for i, y_i in enumerate(y):\n",
    "    #     li = list(range(10))\n",
    "    #     li.remove(y_i)\n",
    "    #     j = np.random.choice(li)\n",
    "    #     y_rnd[i] = j\n",
    "\n",
    "    # x_neg = overlay_y_on_x(x, y_rnd)\n",
    "    # # for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
    "    # #     visualize_sample(data, name)\n",
    "    \n",
    "    net.train(li_epochs=[10000, 10000], li_lr=[8e-3, 8e-3])#x_pos, x_neg)\n",
    "\n",
    "    print('train accu:', 100. * net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "    x_te, y_te = next(iter(test_loader))\n",
    "    # max pooling for x_te\n",
    "    x_te = x_te.view(-1, 28, 28)\n",
    "    x_te = F.max_pool2d(x_te, 2, 2)\n",
    "    x_te = x_te.view(-1, 14*14)\n",
    "    x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "\n",
    "    print('test accu:', 100. * net.predict(x_te).eq(y_te).float().mean().item())\n",
    "\n",
    "    print(net.predict(x_te)[:30], y_te[:30])\n",
    "\n",
    "    plt.plot(net.layers[0].lossli)\n",
    "    plt.plot(net.layers[1].lossli)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "# max pooling for x\n",
    "x = x.view(-1, 28, 28)\n",
    "x = F.max_pool2d(x, 2, 2)\n",
    "x = x.view(-1, 14*14)\n",
    "\n",
    "x, y = x.cuda(), y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accu: 95.99199891090393\n",
      "test accu: 95.53999900817871\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1], device='cuda:0') tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('train accu:', 100. * net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(test_loader))\n",
    "# max pooling for x_te\n",
    "x_te = x_te.view(-1, 28, 28)\n",
    "x_te = F.max_pool2d(x_te, 2, 2)\n",
    "x_te = x_te.view(-1, 14*14)\n",
    "x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "\n",
    "print('test accu:', 100. * net.predict(x_te).eq(y_te).float().mean().item())\n",
    "\n",
    "print(net.predict(x_te)[:30], y_te[:30])\n",
    "\n",
    "plt.plot(net.layers[0].lossli)\n",
    "plt.plot(net.layers[1].lossli)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'net_nobias.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 196])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('net.pth').layers[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-11.2923, -17.1865,  -9.5713,  -4.9747,  -4.8163,  -9.6867, -13.6628,\n",
       "        -10.8079, -10.1169,  -9.0425,  -8.1762, -16.6031, -13.3758, -11.9262,\n",
       "         -9.2174, -13.3813,  -7.0954,  -8.1071, -13.8384, -12.7562,  -8.0672,\n",
       "        -11.7788, -16.3591, -11.4872,  -8.1673, -13.3436, -13.1905, -15.3531,\n",
       "         -9.9936, -12.6681,  -6.9744, -15.8288,  -8.5355, -17.3009, -16.0376,\n",
       "        -11.5817,  -9.9613,  -8.3555,  -7.9277,  -8.0261, -17.1849, -11.7347,\n",
       "         -8.1291, -11.9416,  -8.5231,  -8.3930, -15.4127, -20.2512, -10.4031,\n",
       "         -9.8732, -13.0973, -11.6343,  -3.6198,  -7.3003, -13.3656,  -9.7987,\n",
       "        -12.7610, -11.2629,  -8.2844,  -7.2672,  -7.7224, -13.5735, -10.5594,\n",
       "        -19.5380, -11.7201, -14.1424, -13.9656,  -8.6002,  -9.3627, -13.3626,\n",
       "        -13.5433,  -9.9958, -14.2696,  -9.7691,  -5.0885, -14.4905, -17.0730,\n",
       "        -10.1274,  -8.7416, -12.3818,  -9.8547, -10.6399,  -9.2777,  -6.1891,\n",
       "        -12.7048, -12.2965,  -8.3672, -10.5226, -11.5235,  -5.4865,  -9.6756,\n",
       "        -12.5973,  -7.7995, -16.4126, -17.3466, -17.9931,  -9.5212, -18.2594,\n",
       "        -12.2591, -18.3500, -14.2726,  -8.8125,  -9.4385, -12.3877, -10.4386,\n",
       "         -8.3310, -10.8222,  -8.8382,  -9.7299,  -4.4080, -10.3687, -13.2978,\n",
       "        -12.4906,  -9.0311,  -8.6395,  -7.3180, -14.9949, -14.2421,  -8.6008,\n",
       "        -10.6816, -14.6568, -14.5112,  -6.7764, -13.1388, -13.1103, -12.8971,\n",
       "        -15.3234, -10.6424], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('net.pth').layers[0].bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
