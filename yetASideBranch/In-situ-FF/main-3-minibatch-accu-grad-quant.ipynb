{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:16:54<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:07:18<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accu: 96.44799828529358\n",
      "test accu: 94.96999979019165\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 8, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1], device='cuda:0') tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from train_utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "batchsize = 100\n",
    "#================================\n",
    "# Quantization levels\n",
    "#================================\n",
    "img_half_level = 4\n",
    "weight_bit = 8 \n",
    "output_bit = 6\n",
    "isint = 0\n",
    "clamp_std = 0\n",
    "noise_scale = 5e-2\n",
    "\n",
    "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
    "\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST('./data/', train=True,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MNIST('./data/', train=False,\n",
    "              download=True,\n",
    "              transform=transform),\n",
    "        batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def overlay_y_on_x(x, y):\n",
    "    \"\"\"\n",
    "    Replace the first 10 pixels of data [x] with one-hot-encoded label [y]\n",
    "    \"\"\"\n",
    "    x_ = x.clone()\n",
    "    x_[:, :10] *= 0.0\n",
    "    x_[range(x.shape[0]), y] = x.max()\n",
    "    return x_\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
    "\n",
    "    def predict(self, x): \n",
    "        # 这个predict方法是理解ff方法的关键，它不是像普通的predict方法一样，输入一个样本，输出一个长度为num_cls的softmax预测向量\n",
    "        # 而是一个样本反复输入这个网络num_cls次，把每种带标签的可能都计算一个goodness，也就是这个数据是好数据的可能性，找出最高goodness的就是预测类别\n",
    "        goodness_per_label = []\n",
    "        for label in range(10): # 对每一个标签进行预测\n",
    "            h = overlay_y_on_x(x, label) # h是输入x和标签label的叠加\n",
    "            goodness = [] # goodness是一个列表，里面存放了每一层的结果向量的均方\n",
    "            for layer in self.layers: # 对每一层进行前传\n",
    "                h = layer(h) # h是每一层的输出\n",
    "                goodness += [h.pow(2).mean(1)] # goodness是每一层的结果向量的均方。h.pow(2)是h的每一个元素的平方，mean(1)是对每一行求均值\n",
    "            goodness_per_label += [sum(goodness).unsqueeze(1)] # goodness_per_label是每一层的结果向量的均方的和\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1) # goodness_per_label是每一层的结果向量的均方的和的列表\n",
    "        return goodness_per_label.argmax(1) # 返回的是goodness_per_label中每一行最大值的索引，也就是说，返回的是每一行最大值的列索引\n",
    "\n",
    "    def train(self, li_epochs, li_lr): #, x_pos, x_neg): # 这个train方法是对整个网络进行训练，训练的目标是让正样本的结果向量的均方上升，负样本的结果向量的均方下降\n",
    "        x, y = next(iter(train_loader))\n",
    "        x, _ = my.data_quantization_sym(x, half_level=img_half_level)\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        x_pos = overlay_y_on_x(x, y)\n",
    "        # rnd = torch.randperm(x.size(0)) # 生成一个从0到n-1的随机整数序列。\n",
    "        # x_neg = overlay_y_on_x(x, y[rnd])\n",
    "        y_rnd = y.clone()\n",
    "        for i, y_i in enumerate(y):\n",
    "            li = list(range(10))\n",
    "            li.remove(y_i)\n",
    "            j = np.random.choice(li)\n",
    "            y_rnd[i] = j\n",
    "\n",
    "        x_neg = overlay_y_on_x(x, y_rnd)\n",
    "\n",
    "        h_pos, h_neg = x_pos, x_neg # h_pos和h_neg是正样本和负样本的输入\n",
    "        for i, layer in enumerate(self.layers): # 对每一层进行训练\n",
    "            print('training layer', i, '...') # 这里的i是层数\n",
    "            h_pos, h_neg = layer.train(h_pos, h_neg, num_epochs=li_epochs[i], lr=li_lr[i]) # 对每一层进行训练，得到了正样本和负样本的结果向量，这个结果向量是该层的输出，也是下一层的输入\n",
    "            # 也就是说，这个训练的过程中，正样本在前传过程中得到的每一层输出都被认为是正的，负样本在前传过程中得到的每一层输出都被认为是负的，也就是说，出身决定一切\n",
    "\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = Adam(self.parameters(), lr=8e-3)\n",
    "        self.threshold = 2.0\n",
    "        # self.num_epochs = num_epochs # 训练的次数是1000次\n",
    "        # self.linear = my.Linear_quant_noise(in_features, out_features, weight_bit=weight_bit, output_bit=output_bit, isint=isint, clamp_std=clamp_std, noise_scale=noise_scale, bias=True)\n",
    "        self.weight_bit = weight_bit\n",
    "        self.output_bit = output_bit\n",
    "        self.isint = 0\n",
    "        self.clamp_std = 0\n",
    "        self.noise_scale = 0\n",
    "        self.weight_half_level = 2 ** weight_bit / 2 - 1\n",
    "        self.output_half_level = 2 ** output_bit / 2 - 1\n",
    "        # self.linear = nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "        self.lossli = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape\", x.shape)\n",
    "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)  # 这个是对输入做了归一化，使得输入的模长为1，这在论文里有解释\n",
    "        # print(\"x_norm.shape\", x_direction.shape)\n",
    "        # x_direction = x / x.max()\n",
    "        self.weight_, self.bias_ = my.Weight_Quant_Noise.apply(self.weight, self.bias,\n",
    "                                            self.weight_half_level, \n",
    "                                            self.isint, self.clamp_std,\n",
    "                                            self.noise_scale\n",
    "                                            )\n",
    "        # self.weight_, self.bias_ = self.weight, self.bias\n",
    "        x_direction = self.relu(\n",
    "            # torch.mm(x_direction, self.weight.T) +\n",
    "            # self.bias.unsqueeze(0) # 这个是对输入做了最基本的前向传播，得到了结果向量4\n",
    "            # self.linear(x_direction)\n",
    "            F.linear(x_direction, self.weight_, self.bias_)\n",
    "            ) # 注意，在前传之后，随即使用了relu激活函数，这意味着每一层的所有激活值都是非负的\n",
    "        \n",
    "        x = my.Feature_Quant.apply(x, self.output_half_level, self.isint)\n",
    "        # print(\"x_direction.shape\", x_direction.shape)\n",
    "\n",
    "        return x_direction\n",
    "\n",
    "    def train(self, x_pos, x_neg, num_epochs=4000000, lr=8e-3):\n",
    "        opt = Adam(self.parameters(), lr=lr)\n",
    "        # 训练其实就是对每一层分别进行训练，训练的目标是让正样本的结果向量的均方上升，负样本的结果向量的均方下降\n",
    "        # 每一层的forward方法定义如上一个函数，这里的train方法定义了训练的过程\n",
    "        # print(\"x_pos.shape\", x_pos.shape)\n",
    "        for i in tqdm(range(num_epochs)):\n",
    "            # minibatch\n",
    "            bz = 100\n",
    "            for j in range(0, x_pos.size(0), bz):\n",
    "                \n",
    "                # print(j)\n",
    "                # mask = torch.randperm(x_pos.size(0))[j: j+bz]\n",
    "                # mask = [j: j+bz]\n",
    "                h_pos = x_pos[j: j+bz] # 随机采样1000个正样本\n",
    "                h_neg = x_neg[j: j+bz] # 随机采样1000个负样本\n",
    "\n",
    "\n",
    "                # for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
    "                #     visualize_sample(data, name)\n",
    "                \n",
    "                # print(self.forward(x_pos).pow(2), self.forward(x_pos).pow(2).shape)\n",
    "                g_pos = self.forward(h_pos).pow(2).mean(1) # g_pos 是正样本x_pos在该层前向传播得到的结果向量的均方\n",
    "                g_neg = self.forward(h_neg).pow(2).mean(1) # g_neg 是负样本x_neg在该层前向传播得到的结果向量的均方\n",
    "                # 论文关于使用L2范数来度量的理由：\n",
    "                # There are two main reasons for using the squared length of the activity vector as the goodness function.\n",
    "                # First, it has very simple derivatives. Second, layer normalization removes all trace of the goodness.\n",
    "                \n",
    "                # The following loss pushes pos (neg) samples to\n",
    "                # values larger (smaller) than the self.threshold.\n",
    "                # 随着训练过程，loss下降，g_pos将上升，g_neg将下降\n",
    "                loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                    -g_pos + self.threshold,\n",
    "                    g_neg - self.threshold]))).mean() # loss = [log(1+exp(-(g_pos-threshold))) + log(1+exp(g_neg-threshold))] / 2\n",
    "                # print(loss)\n",
    "                self.lossli.append(loss.item())  # save loss\n",
    "                \n",
    "                # this backward just compute the derivative and hence\n",
    "                # is not considered backpropagation.\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                # self.opt.step()\n",
    "                # self.opt.zero_grad()\n",
    "                    \n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "            # if i<4:\n",
    "            #     # print(a)\n",
    "            #     # 查看累积的梯度值\n",
    "            #     # print(self.parameters())\n",
    "            #     for param in self.parameters():\n",
    "            #         print(param.grad)\n",
    "                # print(loss)\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # 关于这里为什么能够work：\n",
    "            # 1. loss是权重的函数，loss的核心思想是让g_pos上升，g_neg下降\n",
    "            # 2. g_pos和g_neg是x_pos和x_neg的函数，x_pos和x_neg反映了客观世界，是这样要学习的对象。有了x_pos和x_neg，就能够计算出g_pos和g_neg，有了g_pos和g_neg，就能够计算出loss\n",
    "            # 3. 通过loss.backward()，计算loss对权重的梯度，使得loss下降，g_pos上升，g_neg下降\n",
    "            # 4. 通过self.opt.step()，更新了self.weight和self.bias\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
    "\n",
    "    \n",
    "# def visualize_sample(data, name='', idx=0):\n",
    "#     reshaped = data[idx].cpu().reshape(28, 28)\n",
    "#     plt.figure(figsize = (4, 4))\n",
    "#     plt.title(name)\n",
    "#     plt.imshow(reshaped, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(1234)\n",
    "    train_loader, test_loader = MNIST_loaders(train_batch_size=50000, test_batch_size=10000)\n",
    "\n",
    "    net = Net([784, 500, 500])\n",
    "    x, y = next(iter(train_loader))\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    # x_pos = overlay_y_on_x(x, y)\n",
    "    # # rnd = torch.randperm(x.size(0)) # 生成一个从0到n-1的随机整数序列。\n",
    "    # # x_neg = overlay_y_on_x(x, y[rnd])\n",
    "    # y_rnd = y.clone()\n",
    "    # for i, y_i in enumerate(y):\n",
    "    #     li = list(range(10))\n",
    "    #     li.remove(y_i)\n",
    "    #     j = np.random.choice(li)\n",
    "    #     y_rnd[i] = j\n",
    "\n",
    "    # x_neg = overlay_y_on_x(x, y_rnd)\n",
    "    # # for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
    "    # #     visualize_sample(data, name)\n",
    "    \n",
    "    net.train(li_epochs=[10000, 10000], li_lr=[8e-3, 8e-3])#x_pos, x_neg)\n",
    "\n",
    "    print('train accu:', 100. * net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "    x_te, y_te = next(iter(test_loader))\n",
    "    x_te, y_te = x_te.cuda(), y_te.cuda()\n",
    "\n",
    "    print('test accu:', 100. * net.predict(x_te).eq(y_te).float().mean().item())\n",
    "\n",
    "    print(net.predict(x_te)[:30], y_te[:30])\n",
    "\n",
    "    plt.plot(net.layers[0].lossli)\n",
    "    plt.plot(net.layers[1].lossli)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
